{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc114b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import json\n",
    "import tqdm\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Any, List\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from environments.maze_env import MazeEnvironment\n",
    "from environments.syllogism_env import SyllogismEnvironment\n",
    "from environments.gsm8k import GSM8KEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31e4cb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path) -> Dict[str, Any]:\n",
    "    with open(config_path, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "def get_environment(config: Dict[str, Any]) -> Any:\n",
    "    # Need to maintain compat with classes taking dict\n",
    "    env_config = config.get('environment', {})\n",
    "    if not env_config:\n",
    "        if config.get('data', {}).get('dataset_name') == 'openai/gsm8k':\n",
    "            return GSM8KEnvironment(config)\n",
    "            \n",
    "    name = env_config.get('name')\n",
    "    if name == 'gsm8k':\n",
    "        return GSM8KEnvironment(config)\n",
    "    elif name == 'maze':\n",
    "        return MazeEnvironment(config)\n",
    "    elif name == 'syllogism':\n",
    "        return SyllogismEnvironment(config)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown environment: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d16a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_gen.generate_sft_data import DataGenConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1324a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Maze dataset with config: MazeConfig(min_dist=2, max_dist=8, min_grid_size=5, max_grid_size=10, seed=42, size=10)\n",
      "Increasing maze generation size from 10 to 200 to match max_samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 200/200 [00:00<00:00, 13377.25 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: Qwen/Qwen2-7B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "raw_config = load_config(\"/home/rlvr_experiments/configs/data_gen_maze.yaml\")\n",
    "    # Parse into typed config\n",
    "config = DataGenConfig.from_dict(raw_config)\n",
    "\n",
    "# Initialize Environment\n",
    "# We pass raw_config to keep compatibility with environment classes that expect a dict\n",
    "env = get_environment(raw_config)\n",
    "dataset = env.get_dataset(raw_config)\n",
    "\n",
    "# Load Model\n",
    "print(f\"Loading model: {config.model.name_or_path}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model.name_or_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model.name_or_path,\n",
    "    torch_dtype=config.model.dtype,\n",
    "    device_map=config.model.device_map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "784e4cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant. You are given a problem and its correct solution. \n",
      "Your task is to generate the step-by-step reasoning that leads to this solution.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Problem:\n",
      "Navigate from '3' (start) to 'z' (goal):\n",
      "\n",
      "```\n",
      ">>>>>>\n",
      ">e>e>>\n",
      ">eeee>\n",
      ">ezee>\n",
      ">ee3>>\n",
      ">>>>>>\n",
      "```\n",
      "Legend: '>' = Wall, 'e' = Passage\n",
      "\n",
      "What is the minimum number of steps to reach the goal?\n",
      "Give only the number of steps as your final answer, no other text or formatting.\n",
      "\n",
      "Correct Solution:\n",
      "2\n",
      "\n",
      "Please explain the reasoning step-by-step to arrive at this solution.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i, item in tqdm.tqdm(enumerate(dataset), total=min(len(dataset), 2)):\n",
    "    # item['prompt'] is a list of {'role':..., 'content':...}\n",
    "    # item['answer'] is the ground truth\n",
    "    \n",
    "    # Extract the original user query\n",
    "    user_content = next((msg['content'] for msg in item['prompt'] if msg['role'] == 'user'), None)\n",
    "    system_content = next((msg['content'] for msg in item['prompt'] if msg['role'] == 'system'), None)\n",
    "    \n",
    "    ground_truth = item['answer']\n",
    "    \n",
    "    # Construct a meta-prompt to get the reasoning\n",
    "    \n",
    "    # Get tokens from config\n",
    "    think_start = config.sft.think_start_token\n",
    "    think_end = config.sft.think_end_token\n",
    "    answer_start = config.sft.answer_start_token\n",
    "    answer_end = config.sft.answer_end_token\n",
    "    system_prompt_tmpl = config.sft.system_prompt\n",
    "    \n",
    "    # If the template contains format placeholders for tokens, format them.\n",
    "    try:\n",
    "            system_msg = system_prompt_tmpl.format(think_start=think_start, think_end=think_end, answer_start=answer_start, answer_end=answer_end)\n",
    "    except KeyError:\n",
    "            # Fallback if user prompt doesn't match format keys\n",
    "            system_msg = system_prompt_tmpl\n",
    "\n",
    "    meta_prompt = [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\", \"content\": f\"Problem:\\n{user_content}\\n\\nCorrect Solution:\\n{ground_truth}\\n\\nPlease explain the reasoning step-by-step to arrive at this solution.\"}\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(meta_prompt, tokenize=False, add_generation_prompt=True)\n",
    "    print(text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8377f056",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "# Force the model to start with the think_start token\n",
    "think_ids = tokenizer(think_start, add_special_tokens=False).input_ids\n",
    "# Ensure it's on the same device\n",
    "think_tensor = torch.tensor([think_ids], device=model.device)\n",
    "\n",
    "# Concatenate\n",
    "current_ids = inputs.input_ids\n",
    "current_mask = inputs.attention_mask\n",
    "\n",
    "input_ids = torch.cat([current_ids, think_tensor], dim=1)\n",
    "# Extend attention mask\n",
    "attention_mask = torch.cat([current_mask, torch.ones((1, len(think_ids)), device=model.device)], dim=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "# Decode only the NEW tokens\n",
    "generated_ids = outputs[0][input_ids.shape[1]:]\n",
    "generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b81c355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Initially, we start by moving 1 step in any direction. We can go left, right, up, or down.</think>\\n\\n<answer>2</answer>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19a8236",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
