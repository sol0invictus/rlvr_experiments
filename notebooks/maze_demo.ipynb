{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Maze Environment Visualization and Model Demo\n",
                "\n",
                "This notebook demonstrates how to:\n",
                "1. Load the Maze environment configuration.\n",
                "2. Generate examples from the environment.\n",
                "3. Visualize the raw prompts sent to the model.\n",
                "4. Run the model to see its formatted output."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/root/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded config for environment: maze\n"
                    ]
                }
            ],
            "source": [
                "import yaml\n",
                "import torch\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "from environments.maze_env import MazeEnvironment\n",
                "\n",
                "# 1. Load Configuration\n",
                "config_path = '../configs/config_maze.yaml'\n",
                "with open(config_path, 'r') as f:\n",
                "    config = yaml.safe_load(f)\n",
                "\n",
                "print(f\"Loaded config for environment: {config['environment']['name']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Generating Maze dataset with config: MazeConfig(min_dist=5, max_dist=10, min_grid_size=5, max_grid_size=10, seed=42, size=500)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Map: 100%|██████████| 500/500 [00:00<00:00, 5819.17 examples/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataset generated with 500 examples.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "# 2. Initialize Environment and Dataset\n",
                "env = MazeEnvironment(config)\n",
                "dataset = env.get_dataset(config)\n",
                "\n",
                "print(f\"Dataset generated with {len(dataset)} examples.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- Sample Index: 124 ---\n",
                        "\n",
                        "System Prompt:\n",
                        "You are a helpful assistant. You must output your reasoning steps within <think></think> tags and your final answer within <answer></answer> tags.\n",
                        "            You goal is to find the shortest path from start to end, you can move steps in passage, while avoiding the Walls.\n",
                        "\n",
                        "User Prompt (The Maze):\n",
                        "Navigate from ':' (start) to 'Q' (goal):\n",
                        "\n",
                        "```\n",
                        "mmmmmmmm\n",
                        "m=m=:==m\n",
                        "m===m=mm\n",
                        "m=m====m\n",
                        "m=m==m=m\n",
                        "m=m==mmm\n",
                        "mmm==Q=m\n",
                        "mmmmmmmm\n",
                        "```\n",
                        "Legend: 'm' = Wall, '=' = Passage\n",
                        "\n",
                        "What is the minimum number of steps to reach the goal?\n",
                        "Give only the number of steps as your final answer, no other text or formatting.\n",
                        "\n",
                        "Ground Truth Answer:\n",
                        "8\n"
                    ]
                }
            ],
            "source": [
                "# 3. Visualize a Sample\n",
                "import random\n",
                "\n",
                "idx = random.randint(0, len(dataset)-1)\n",
                "sample = dataset[idx]\n",
                "\n",
                "print(f\"--- Sample Index: {idx} ---\\n\")\n",
                "print(\"System Prompt:\")\n",
                "print(sample['prompt'][0]['content'])\n",
                "print(\"\\nUser Prompt (The Maze):\")\n",
                "print(sample['prompt'][1]['content'])\n",
                "print(\"\\nGround Truth Answer:\")\n",
                "print(sample['answer'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading model...\n",
                        "Model loaded.\n"
                    ]
                }
            ],
            "source": [
                "# 4. Load Model (Optional - can skip if no GPU or just want to see data)\n",
                "print(\"Loading model...\")\n",
                "model_name =\"/home/rlvr_experiments/outputs/maze-sft\"\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_name, \n",
                "    torch_dtype='auto', \n",
                "    device_map='auto'\n",
                ")\n",
                "print(\"Model loaded.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[{'content': 'You are a helpful assistant. You must output your reasoning steps within <think></think> tags and your final answer within <answer></answer> tags.\\n            You goal is to find the shortest path from start to end, you can move steps in passage, while avoiding the Walls.',\n",
                            "  'role': 'system'},\n",
                            " {'content': \"Navigate from 'g' (start) to 'V' (goal):\\n\\n```\\niiiiiiiiii\\ni>>>>>>i>i\\ni>>>>>iiii\\ni>>>>>>>>i\\ni>>>>>>>>i\\ni>i>>ii>>i\\ni>>>V>iiii\\ni>>>i>>>ii\\ni>i>i>i>gi\\niiiiiiiiii\\n```\\nLegend: 'i' = Wall, '>' = Passage\\n\\nWhat is the minimum number of steps to reach the goal?\\nGive only the number of steps as your final answer, no other text or formatting.\",\n",
                            "  'role': 'user'}]"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "sample['prompt']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Running inference on the sample above...\n",
                        "\n",
                        "<|im_start|>system\n",
                        "You are a helpful assistant. You must output your reasoning steps within <think></think> tags and your final answer within <answer></answer> tags.\n",
                        "            You goal is to find the shortest path from start to end, you can move steps in passage, while avoiding the Walls.<|im_end|>\n",
                        "<|im_start|>user\n",
                        "Navigate from ':' (start) to 'Q' (goal):\n",
                        "\n",
                        "```\n",
                        "mmmmmmmm\n",
                        "m=m=:==m\n",
                        "m===m=mm\n",
                        "m=m====m\n",
                        "m=m==m=m\n",
                        "m=m==mmm\n",
                        "mmm==Q=m\n",
                        "mmmmmmmm\n",
                        "```\n",
                        "Legend: 'm' = Wall, '=' = Passage\n",
                        "\n",
                        "What is the minimum number of steps to reach the goal?\n",
                        "<|im_end|>\n",
                        "<|im_start|>assistant\n",
                        "\n",
                        "--- Model Response ---\n",
                        "<think>To find the minimum number of steps required to reach the goal, we need to consider all possible paths between the start and goal positions.</th>\n",
                        "<answer>4</answer></think><answer>5</answer><|im_end|>\n"
                    ]
                }
            ],
            "source": [
                "# 5. Run Inference\n",
                "def generate_response(prompt_messages):\n",
                "\n",
                "    text = tokenizer.apply_chat_template(prompt_messages, tokenize=False, add_generation_prompt=True)\n",
                "    text = text.replace(\"Give only the number of steps as your final answer, no other text or formatting.\",\"\")\n",
                "\n",
                "    print(text)\n",
                "    inputs = tokenizer(text, \n",
                "    return_tensors='pt').to(model.device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model.generate(\n",
                "            **inputs, \n",
                "            max_new_tokens=config['generation']['max_completion_length'],\n",
                "            do_sample=True,\n",
                "        )\n",
                "    \n",
                "    # Decode only the new tokens\n",
                "    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=False)\n",
                "    return response\n",
                "\n",
                "print(\"Running inference on the sample above...\\n\")\n",
                "response = generate_response(sample['prompt'])\n",
                "\n",
                "print(\"--- Model Response ---\")\n",
                "print(response)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "dict_keys(['question', 'answer', 'metadata', 'prompt'])"
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "sample.keys()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'6'"
                        ]
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "sample[\"answer\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
