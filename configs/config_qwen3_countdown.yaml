# ============================================================================
# GRPO Training: Countdown task on Qwen3-4B-Base
# ============================================================================
# Usage: python train_grpo.py configs/config_qwen3_countdown.yaml
#
# IMPORTANT: Run SFT warmup first to teach the model <think>/<answer> format:
#   python data_gen/generate_sft_warmup.py
#   python train_sft.py configs/config_qwen3_sft_warmup.yaml
# Then change model.name_or_path below to results/qwen3_sft_warmup
#
# Designed for a single A100 (80GB). 500 steps ~2-3 hours.
# ============================================================================

model:
  name_or_path: Qwen/Qwen3-4B-Base  # Change to results/qwen3_sft_warmup after warmup
  torch_dtype: bfloat16
  device_map: auto

# --- Reasoning Gym Environment ---
environment:
  name: reasoning_gym
  task_name: countdown
  samples: 5000
  task_params:
    min_numbers: 4
    max_numbers: 5
    min_target: 50
    max_target: 200

seed: 42

# --- Training ---
training:
  output_dir: results/grpo_qwen3_countdown
  learning_rate: 5.0e-6
  gradient_accumulation_steps: 8
  num_train_epochs: 2
  bf16: true
  logging_steps: 10
  save_strategy: steps
  save_steps: 100
  max_steps: 500
  report_to: []

# --- Generation (GRPO rollouts) ---
generation:
  max_completion_length: 1024
  num_generations: 4       # Reduced from 8 to fit 4B model on A100
  max_prompt_length: 512
  temperature: 0.7

# --- System Prompt ---
system_prompt: |
  You are a helpful assistant. Think step by step inside <think>...</think> tags, then provide your final answer inside <answer>...</answer> tags.
